# -*- coding: utf-8 -*-
"""SoC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N_dclcqawmvNTw-Ijrj4Q1x_oyH-8V8u
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset
file_path = '/content/TripB01.csv.csv'
df = pd.read_csv(file_path, encoding='ISO-8859-1', delimiter=';')

# Select relevant columns
selected_columns = [
    "Battery Voltage [V]",
    "Battery Current [A]",
    "Battery Temperature [°C]",
    "Ambient Temperature [°C]",
    "SoC [%]"
]
dataset = df[selected_columns]

# Drop rows with missing values
dataset = dataset.dropna()

# Split the dataset into features (X) and target variable (y)
X = dataset.drop('SoC [%]', axis=1)
y = dataset['SoC [%]']

# Set up k-fold cross-validation with 5 folds
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Define a range of values for the number of trees in the Random Forest
n_estimators_list = [20, 50, 75, 100]

# Initialize lists to store results
results = []

# Iterate over each value of n_estimators
for n_estimators in n_estimators_list:
    print(f"Evaluating RandomForestRegressor with {n_estimators} trees...")

    # Initialize lists to store metrics for this configuration
    mae_scores = []
    mse_scores = []
    r2_scores = []

    # Perform k-fold cross-validation
    for train_index, val_index in kf.split(X):
        X_train, X_val = X.iloc[train_index], X.iloc[val_index]
        y_train, y_val = y.iloc[train_index], y.iloc[val_index]

        # Define the model with the current number of trees
        rf_regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=42)

        # Train the model
        rf_regressor.fit(X_train, y_train)

        # Predict on the validation data
        y_pred = rf_regressor.predict(X_val)

        # Calculate evaluation metrics
        mae = mean_absolute_error(y_val, y_pred)
        mse = mean_squared_error(y_val, y_pred)
        r2 = r2_score(y_val, y_pred)

        # Append metrics to lists
        mae_scores.append(mae)
        mse_scores.append(mse)
        r2_scores.append(r2)

    # Calculate average metrics for this configuration
    avg_mae = np.mean(mae_scores)
    avg_mse = np.mean(mse_scores)
    avg_r2 = np.mean(r2_scores)

    # Store results
    results.append({
        'n_estimators': n_estimators,
        'avg_mae': avg_mae,
        'avg_mse': avg_mse,
        'avg_r2': avg_r2,
        'mae_std': np.std(mae_scores),
        'mse_std': np.std(mse_scores),
        'r2_std': np.std(r2_scores)
    })

# Print results
print("\nHyperparameter Tuning Results:")
for result in results:
    print(f"Number of Trees: {result['n_estimators']}")
    print(f"Average Mean Absolute Error: {result['avg_mae']:.2f} ± {result['mae_std']:.2f}")
    print(f"Average Mean Squared Error: {result['avg_mse']:.2f} ± {result['mse_std']:.2f}")
    print(f"Average R² Score: {result['avg_r2']:.2f} ± {result['r2_std']:.2f}")
    print('-' * 30)

